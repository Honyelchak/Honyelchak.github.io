import{_ as e}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as a,c as i,f as o}from"./app-2Xbp2Heq.js";const n="/assets/冯诺依曼模型-aoQUe3e6.png",s="/assets/image-20230403110911150-6yG-asvW.png",l="/assets/image-20230403111534077-3rZCscgc.png",c="/assets/image-20230403195048414-DS__qcD5.png",t="/assets/image-20230502214909630-30DOow0x.png",d="/assets/72ab76ba697e470b8ceb14d5fc5688d9-NqgpCyHH.png",p="data:image/png;base64,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",r="/assets/image-20230429222638960-ZySCGEKw.png",h="/assets/v2-aa10bd85f52845f4de8ee3b095ffac3e_720w-Izm9u6AK.webp",u="/assets/I_O中断-J_nko8Gi.png",g="/assets/传统文件传输-a6YrJ3r8.png",b="/assets/image-20230322205221612-Fblx9Nqy.png",m="/assets/image-20230322210046826-qKH5TfXS.png",k={},f=o('<h1 id="一、硬件结构" tabindex="-1"><a class="header-anchor" href="#一、硬件结构" aria-hidden="true">#</a> 一、硬件结构</h1><h2 id="_1-2-硬件基本构成" tabindex="-1"><a class="header-anchor" href="#_1-2-硬件基本构成" aria-hidden="true">#</a> 1.2 硬件基本构成</h2><p>最重要的是定义计算机基本结构为 5 个部分，分别是<strong>运算器、控制器、存储器、输入设备、输出设备</strong>，这 5 个部分也被称为<strong>冯诺依曼模型</strong>。</p><ul><li><p>运算器、控制器是在CPU中的。</p></li><li><p>存储器就是常见的内存。</p></li><li><p>存储单元和输入输出设备要与中央处理器打交道的话，离不开总线。</p><figure><img src="'+n+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure></li></ul><h3 id="cpu" tabindex="-1"><a class="header-anchor" href="#cpu" aria-hidden="true">#</a> CPU</h3><p>中央处理器也就是我们常说的 CPU，32 位和 64 位 CPU 最主要区别在于一次能计算多少字节数据：</p><p>CPU 的位宽，代表的是 CPU 一次可以计算（运算）的数据量。CPU 位宽越大，可以计算的数值就越大。</p><ul><li>32 位 CPU 一次可以计算 4 个字节；</li><li><strong>64 位 CPU 一次可以计算 8 个字节；</strong></li></ul><p>CPU 内部还有一些组件，常见的有<strong>寄存器、控制单元和逻辑运算单元</strong>等。</p><ul><li>控制单元负责控制 CPU 工作</li><li>逻辑运算单元负责计算</li><li>寄存器可以分为多种类，每种寄存器的功能又不尽相同。 <ul><li><em>通用寄存器</em>，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。</li><li><em>程序计数器</em>，用来存储 CPU 要执行下一条指令「所在的内存地址」，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令「的地址」。</li><li><em>指令寄存器</em>，用来存放当前正在执行的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里。</li></ul></li></ul><h3 id="内存" tabindex="-1"><a class="header-anchor" href="#内存" aria-hidden="true">#</a> 内存</h3><p>内存的地址是从 0 开始编号的，然后自增排列，最后一个地址为内存总字节数 - 1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。</p><h3 id="总线" tabindex="-1"><a class="header-anchor" href="#总线" aria-hidden="true">#</a> 总线</h3><p>总线是用于 <code>CPU</code> 和<code>内存以及其他设备</code>之间的通信，总线可分为 3 种：</p><ul><li><code>地址总线</code>，用于指定 CPU 将要操作的内存地址；</li><li><code>数据总线</code>，用于读写内存的数据；</li><li><code>控制总线</code>，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；</li></ul><p>当 CPU 要读写内存数据的时候，一般需要通过下面这三个总线：</p><ul><li>首先要通过「地址总线」来指定内存的地址；</li><li>然后通过「控制总线」控制是读或写命令；</li><li>最后通过「数据总线」来传输数据；</li></ul><h3 id="输入、输出设备" tabindex="-1"><a class="header-anchor" href="#输入、输出设备" aria-hidden="true">#</a> 输入、输出设备</h3><p>输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。期间，如果输入设备是键盘，按下按键时是需要和 CPU 进行交互的，这时就需要用到控制总线了。</p><hr><h3 id="补充" tabindex="-1"><a class="header-anchor" href="#补充" aria-hidden="true">#</a> 补充</h3><h4 id="线路位宽与cpu位宽" tabindex="-1"><a class="header-anchor" href="#线路位宽与cpu位宽" aria-hidden="true">#</a> 线路位宽与CPU位宽</h4><p>数据是如何通过线路传输的呢？其实是通过操作电压，低电压表示 0，高压电压则表示 1。所以一条线路执行串行传输信号，在某时刻只能传递一个信号，即只能传递一位。</p><p>为了避免低效率的串行传输方式，线路的位宽最好一次就能访问到所有的内存地址。</p><p>CPU 想要操作「内存地址」就需要「地址总线」：</p><ul><li>如果地址总线只有 1 条，那每次只能表示 「0 或 1」这两种地址，所以 CPU 能操作的内存地址最大数量为 2（2^1）个（注意，不要理解成同时能操作 2 个内存地址）；</li><li>如果地址总线有 2 条，那么能表示 00、01、10、11 这四种地址，所以 CPU 能操作的内存地址最大数量为 4（2^2）个。</li></ul><p>那么，想要 CPU 操作 4G 大的内存，那么就需要 32 条地址总线，因为 <code>2 ^ 32 = 4G</code>。</p><p>所以，32 位 CPU 最大只能操作 4GB 内存，就算你装了 8 GB 内存条，也没用。而 64 位 CPU 寻址范围则很大，理论最大的寻址空间为 <code>2^64</code>，1GB = 2<sup>10次方，所以64位大概2</sup>34 GB。通常来说 64 位 CPU 的地址总线是 48 位。</p><h4 id="_64-位和-32-位软件" tabindex="-1"><a class="header-anchor" href="#_64-位和-32-位软件" aria-hidden="true">#</a> 64 位和 32 位软件</h4><p>64 位和 32 位软件，实际上<code>代表指令是 64 位还是 32 位的：</code></p><ul><li>如果 32 位指令在 64 位机器上执行，需要一套兼容机制，就可以做到兼容运行了。但是<strong>如果 64 位指令在 32 位机器上执行，就比较困难了，因为 32 位的寄存器存不下 64 位的指令</strong>；</li><li>操作系统其实也是一种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，比如 64 位操作系统，指令也就是 64 位，因此不能装在 32 位机器上。</li></ul><h4 id="todo-cpu位宽、寄存器位宽、操作系统的位宽、线路位宽的关系" tabindex="-1"><a class="header-anchor" href="#todo-cpu位宽、寄存器位宽、操作系统的位宽、线路位宽的关系" aria-hidden="true">#</a> <mark>(TODO)CPU位宽、寄存器位宽、操作系统的位宽、线路位宽的关系</mark></h4><blockquote><p>网络有些“<strong>操作系统的位数，指的是线路位宽，也就是指地址总线的位数</strong>。”的结论是错误的。</p><ul><li>线路位宽是硬件层面决定的，而操作系统是软件。他俩并不是同一个概念。位宽为64的CPU，可以安装32位的操作系统，并兼容32位的应用程序。另外，64位CPU的地址总线，位宽实际上一般是48。</li></ul></blockquote><ul><li><p>CPU位宽：主要指的是CPU一次能够处理的二进制数据位数</p></li><li><p>寄存器位宽：</p></li><li><p>操作系统位宽：</p></li><li><p>线路位宽</p><ul><li>地址总线位宽：决定CPU可直接寻址的内存空间的大小(<code>2 ^ 位宽</code>)，以我个人服务器为例： <ul><li>通过<code>cat /proc/cpuinfo</code>可得到某个核心地址大小的信息</li><li>具体为：<code>address sizes : 46 bits physical, 48 bits virtual</code></li><li>46代表物理地址的位数，即CPU可以寻址的物理内存空间的大小。说白了就是</li></ul></li><li>数据总线位宽：</li></ul></li></ul><p>按照我的理解：</p><ol><li>CPU、寄存器、线路的位宽都是硬件层面决定的，而操作系统的位宽是软件决定的。</li><li>线路位宽或者叫做数据总线其实又分为内部数据总线和外部数据总线，内部数据总线负责寄存器和运算器之间的数据一</li></ol><ul><li>64位的CPU可以安装32位的操作系统，并且可以兼容的运行32位的应用程序。</li></ul><p>地址总线描述的是寻址能力。</p><h2 id="_1-3-存储器的级别" tabindex="-1"><a class="header-anchor" href="#_1-3-存储器的级别" aria-hidden="true">#</a> 1.3 存储器的级别</h2><p>在计算机中，存储器主要有以下几种：</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/存储器的层次关系图.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h3 id="为什么会有cpu缓存" tabindex="-1"><a class="header-anchor" href="#为什么会有cpu缓存" aria-hidden="true">#</a> 为什么会有CPU缓存</h3><p>你可能会好奇为什么有了内存，还需要 CPU Cache？根据摩尔定律，CPU 的访问速度每 18 个月就会翻倍，相当于每年增长 60% 左右，内存的速度当然也会不断增长，但是增长的速度远小于 CPU，平均每年只增长 7% 左右。于是，CPU 与内存的访问性能的差距不断拉大。</p><p>目前，一次内存访问所需时间是200~300个时钟周期，所以为了弥补CPU与内存两者之间的性能差异，就在CPU内部引入了CPU Cache，也称高速缓存。</p><blockquote><p>时钟周期：是计算机中最基本的、最小的时间单位。在一个时钟周期内，CPU仅完成一个最基本的动作。 计算方法： 时钟周期 = 1 /(主频) 单位大概是</p></blockquote><h3 id="cpu缓存" tabindex="-1"><a class="header-anchor" href="#cpu缓存" aria-hidden="true">#</a> CPU缓存</h3><p>分为大小不等的三级缓存， L1 Cache、L2 Cache、L3 Cache</p><blockquote><p>现在大家的电脑都是单CPU多核心，即一个CPU芯片上集成了多个核心。</p><p><strong>一个CPU有多个核心，每个核心都可以处理，</strong></p><p>在多核心的CPU中，每个CPU核心都有各自的L1、L2 cache，而L3是所有核心共享使用的。</p></blockquote><p>在 Linux 系统中，我们可以使用下图的方式来查看各级 CPU Cache 的大小，比如我这手上这台服务器，离 CPU 核心最近的 L1 Cache 是 32KB，其次是 L2 Cache 是 256KB，最大的 L3 Cache 则是 15MB。</p><figure><img src="'+s+'" alt="image-20230403110911150" tabindex="0" loading="lazy"><figcaption>image-20230403110911150</figcaption></figure><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/存储区分级.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><figure><img src="'+l+'" alt="image-20230403111534077" tabindex="0" loading="lazy"><figcaption>image-20230403111534077</figcaption></figure><p>其中，<strong>L1 Cache 通常会分为「数据缓存」和「指令缓存」</strong>，这意味着数据和指令在 L1 Cache 这一层是分开缓存的，上图中的 <code>index0</code> 也就是数据缓存，而 <code>index1</code> 则是指令缓存，它两的大小通常是一样的。</p><p>另外，你也会注意到，L3 Cache 比 L1 Cache 和 L2 Cache 大很多，这是因为 <strong>L1 Cache 和 L2 Cache 都是每个 CPU 核心独有的，而 L3 Cache 是多个 CPU 核心共享的。</strong></p><blockquote><p>A unified cache is <em>a cache that contains both code (instructions) and data</em>， 也就是说Unified是包含数据和指令的缓存</p></blockquote><p>程序执行时，会先将内存中的数据加载到<code>共享的L3 Cache</code>中，再加载到每个核心独有的<code>L2 Cache</code>， 最后进入到最快的 L1 Cache，之后才会被 CPU 读取。</p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/CPU-Cache.png" alt="img" style="zoom:50%;"><h3 id="cpu缓存的数据结构和读取过程" tabindex="-1"><a class="header-anchor" href="#cpu缓存的数据结构和读取过程" aria-hidden="true">#</a> CPU缓存的数据结构和读取过程</h3><p>CPU cache是由很多个Cache Line组成的，<code>Cache Line是CPU从内存读取数据的基本单位</code>，而Cache Line是由各种标志(Tag) + 数据块(Data Block) 组成。</p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存一致性/Cache的数据结构.png" alt="img" style="zoom:50%;"><p>CPU Cache 的数据是从内存中读取过来的，它是以一小块一小块读取数据的，而不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样一小块一小块的数据，称为 <strong>Cache Line（缓存行）</strong>。</p><p>如下图，可以看出L1、L2、L3的cache line都是64字节</p><figure><img src="'+c+'" alt="image-20230403195048414" tabindex="0" loading="lazy"><figcaption>image-20230403195048414</figcaption></figure><p>比如，有一个 <code>int array[100]</code> 的数组，当载入 <code>array[0]</code> 时，由于这个数组元素的大小在内存只占 4 字节，不足 64 字节，CPU 就会<strong>顺序加载</strong>数组元素到 <code>array[15]</code>，意味着 <code>array[0]~array[15]</code> 数组元素都会被缓存在 CPU Cache 中了，因此当下次访问这些数组元素时，会直接从 CPU Cache 读取，而不用再从内存中读取，大大提高了 CPU 读取数据的性能。</p><p>那 CPU 怎么知道要访问的内存数据，是否在 Cache 里？如果在的话，如何找到 Cache 对应的数据呢？</p><h4 id="直接映射cache-direct-mapped-cache" tabindex="-1"><a class="header-anchor" href="#直接映射cache-direct-mapped-cache" aria-hidden="true">#</a> 直接映射Cache (Direct Mapped Cache)</h4><p>CPU是按照coherency_line_size(64字节)去读取CPU缓存的，而在内存中，这一块的数据我们称为内存块(Block)，读取内存的时候需要拿到数据所在内存块的地址。</p><p>直接映射Cache就是将<code>内存块的地址</code>映射在一个<code>CPU Cache Line</code>的地址，至于映射关系实现方式则是使用<code>取模运算</code>，取模运算的结果就是内存块地址对应的CPU缓存块的地址。</p><p><em>CPU</em>与<em>cache</em>之间的数据交换是以&quot;Word&quot;为<em>单位</em>，一个字的</p><hr><h2 id="_1-4-什么是中断" tabindex="-1"><a class="header-anchor" href="#_1-4-什么是中断" aria-hidden="true">#</a> 1.4 什么是中断</h2><p>中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后<code>调用内核中的中断处理程序来响应请求</code>。</p><p>操作系统收到了中断请求，会打断其他进程的运行，所以<strong>中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。</strong></p><p>而且，中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说中断有可能会丢失，所以中断处理程序要短且快。</p><h3 id="软中断" tabindex="-1"><a class="header-anchor" href="#软中断" aria-hidden="true">#</a> 软中断</h3><p>Linux 系统<strong>为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」</strong>。</p><ul><li><strong>上半部直接处理硬件请求，也就是硬中断</strong>，主要是负责耗时短的工作，特点是快速执行；</li><li><strong>下半部是由内核触发，也就说软中断</strong>，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；</li></ul><p>再举一个计算机中的例子，常见的网卡接收网络包的例子。</p><p>网卡收到网络包后，通过 DMA 方式将接收到的数据写入内存，接着会通过<strong>硬件中断</strong>通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来处理该事件，这个事件的处理也是会分成上半部和下半部。</p><p>上部分要做的事情很少，会先禁止网卡中断，避免频繁硬中断，而降低内核的工作效率。接着，内核会触发一个<strong>软中断</strong>，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。</p><p>还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，<strong>而软中断（下半部）是以内核线程的方式执行，并且<code>每一个 CPU 都对应一个软中断内核线程</code></strong>，名字通常为「ksoftirqd/CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 <code>ksoftirqd/0</code></p><h2 id="_1-5-为什么0-1-0-2不等于0-3" tabindex="-1"><a class="header-anchor" href="#_1-5-为什么0-1-0-2不等于0-3" aria-hidden="true">#</a> 1.5 为什么0.1 + 0.2不等于0.3？</h2><h3 id="为什么负数要用补码表示" tabindex="-1"><a class="header-anchor" href="#为什么负数要用补码表示" aria-hidden="true">#</a> 为什么负数要用补码表示？</h3><p>以<code>int</code>类型的数字作为例子，int类型是<code>32</code>位，其中最高位作为<strong>符号位</strong>。</p><ul><li>正数的符号位位0，剩下的31位则表示二进制数据</li><li>负数的符号位为1，剩下的31位是正数的二进制全部取反再加1</li></ul><p>如果负数不是使用补码的方式表示，则在做基本对加减法运算的时候，<strong>还需要多一步操作来判断是否为负数，如果为负数，还得把加法反转成减法，或者把减法反转成加法</strong>。</p><p><strong>而用了补码的表示方式，对于负数的加减法操作，实际上是和正数加减法操作一样的</strong>。</p><blockquote><p>为什么负数要用补码表示？</p></blockquote><p>负数之所以用补码的方式来表示，主要是为了统一和正数的加减法操作一样，毕竟数字的加减法是很常用的一个操作，就不要搞特殊化，尽量以统一的方式来运算。</p><blockquote><p>十进制小数怎么转成二进制？</p></blockquote><p>十进制整数转二进制使用的是「<code>除 2 取余法</code>」，十进制小数使用的是「<code>乘 2 取整法</code>」。</p><blockquote><p>计算机是怎么存小数的？</p></blockquote><p>计算机是以浮点数的形式存储小数的，大多数计算机都是 IEEE 754 标准定义的浮点数格式，包含三个部分：</p><ul><li><p><code>符号位</code>：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；</p></li><li><p><code>指数位</code>：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，指数位的长度越长则数值的表达范围就越大；</p><ul><li>指数位等于</li><li>这个地方存储的是无符号整数，指数有可能是负数，所以这个地方加了一个偏移量127</li></ul></li><li><p><code>尾数位</code>：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且尾数的长度决定了这个数的精度，因此如果要表示精度更高的小数，则就要提高尾数位的长度；</p><ul><li>需要注意的是，一般情况下会把整数部分的1省略掉，不进行存储。</li></ul></li></ul><p>用 32 位来表示的浮点数，则称为单精度浮点数，也就是我们编程语言中的 float 变量，而用 64 位来表示的浮点数，称为双精度浮点数，也就是 double 变量。</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/float.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><blockquote><p>0.1 + 0.2 == 0.3 吗？</p></blockquote><ol><li>在计算机中，对于浮点数的存储，不是我们想象中的那种存储方式。拿float来举例子，float占用4个字节，也就是32位。其中，第一位是符号位，接着8位是指数位，最后23位表示尾数位。</li><li>接下来举个小例子，10.625是如何存储的，首先<code>整数位</code>利用<u>除2法</u>来进行存储，而<code>小数位</code>用<u>乘2法</u>来进行存储，得到<code>1010.101</code>，那么会使用科学计数法来表示该数，即<code>1.010101 x 2^3</code>。之后存储的时候，符号位为0，指数位(3 + 127)为130(<code>10000010</code>), 尾数位只存储<code>0101010000....</code>。最后存储的就是<code>01000001 00101010 00000000 00000000</code></li><li>像上边那个例子存储的时候就不会有精度问题。而如果我们要存储的小数在转换的过程中有循环节的存在或者乘2法凑不到整数，那么存储的时候就会发生精度问题导致实际存储的数值和我们的预期的有偏差，例如：0.1，实际存储是<code>00111101 11001100 11001100 11001100</code>, 转化为10进制之后为<code>9.99999940395355224609375E-2</code>,完全不等于0.1。</li><li>当然不一定是所有的小数都有这样的精度丢失问题，像<code>1/2 1/4 1/8 1/(2^n)</code>这种基本上不会出现这种情况。</li></ol><p>不是的，0.1 和 0.2 这两个数字用二进制表达会是一个一直循环的二进制数，比如 0.1 的二进制表示为 0.0 0011 0011 0011… （0011 无限循环)，对于计算机而言，0.1 无法精确表达，这是浮点数计算造成精度损失的根源。</p><p>因此，IEEE 754 标准定义的浮点数只能根据精度舍入，然后用「近似值」来表示该二进制，那么意味着计算机存放的小数可能不是一个真实值。</p><p>0.1 + 0.2 并不等于完整的 0.3，这主要是因为这两个小数无法用「完整」的二进制来表示，只能根据精度舍入，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数。</p><hr><h1 id="二、内存管理" tabindex="-1"><a class="header-anchor" href="#二、内存管理" aria-hidden="true">#</a> 二、内存管理</h1><h2 id="_2-1-32位寻址4g空间-64位128g" tabindex="-1"><a class="header-anchor" href="#_2-1-32位寻址4g空间-64位128g" aria-hidden="true">#</a> 2.1 32位寻址4G空间，64位128G？</h2><blockquote><p>Going back to a really basic idea, we have 32 bits for our memory addresses. That works out to 2^32 unique combinations of addresses. By convention, each address points to 1 byte of data. Therefore, we can access up to a total 2^32 bytes of data</p></blockquote><ol><li>32位系统中的32表示的是CPU一次能够处理二进制数据位数，而决定CPU寻址空间的是地址总线的宽度，通常为了能够方便、快速的支持并行传输，32位系统一般让地址总线的宽度等于CPU的位数，也就是说CPU寻址的空间就是0~2^32 - 1，也就是说CPU能够访问2^32个地址，又因为每个地址指向一个字节，所以我们可以访问<code>2 ^ 32 bytes</code>，也即4 GB。</li></ol><blockquote><p>补充：32、64位系统中的32、64代表什么？</p><ol><li>32、64位一般表示CPU能够一次性处理的二进制数据位数。</li><li>例如在32位系统中，指令集架构、CPU寄存器位宽、物理内存寻址位数都是32位。</li><li>一般为了并行传输，地址总线、数据总线也都是32位的。</li></ol><p>注意：32位系统中，控制总线位数一般不等于系统的位数，因为实际中并没有那么多控制信号，一般使用8~16位来。</p></blockquote><p><strong>注意一：此处指的寻址空间是物理内存寻址空间</strong></p><ol><li><p>地址总线是CPU和内存进行通信的桥梁，而地址总线位宽越大，CPU能够直接寻址的物理内存空间就越大。</p></li><li><p>逻辑内存地址是操作系统对物理内存的抽象和管理，它通过虚拟内存机制将物理内存映射到逻辑地址，<code>逻辑地址空间理论上可以非常大，甚至是无限大</code>。<mark>就好比在B+树中，通过增加树的层级来容纳更多的数据条数。在分页机制下，可以通过增加页表的层级来让操作系统能够访问到更多的虚拟内存空间</mark></p><blockquote><p>在实际应用中，考虑到层级数量和性能需求，并不会让逻辑地址空间非常大。在B+树种，增加层级会增加索引的查找次数(多次IO)；在分页机制下，增加层级会增加地址转换的开销。</p></blockquote></li></ol><figure><img src="'+t+'" alt="image-20230502214909630" tabindex="0" loading="lazy"><figcaption>image-20230502214909630</figcaption></figure><h2 id="_2-2-为什么要有虚拟内存" tabindex="-1"><a class="header-anchor" href="#_2-2-为什么要有虚拟内存" aria-hidden="true">#</a> 2.2 为什么要有虚拟内存</h2><p>虚拟内存实际上是一种主存的抽象概念，<code>通过CPU的内存管理单元(MMU)将虚拟地址翻译为物理地址</code>，从而可以为进程提供独立的地址空间，同时保护进程的地址空间不被其他进程破坏。它使得应用程序认为它拥有连续的可用内存(一个连续完整的地址空间)，而实际上，它通常是被分割成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上。</p><p>如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。</p><p>于是，这里就引出了两种地址的概念：</p><ul><li>我们程序所使用的内存地址叫做<strong>虚拟内存地址</strong>（<em>Virtual Memory Address</em>）</li><li>实际存在硬件里面的空间地址叫<strong>物理内存地址</strong>（<em>Physical Memory Address</em>）。</li></ul><figure><img src="'+d+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><blockquote><p>操作系统是如何管理虚拟地址与物理地址之间的关系？</p></blockquote><p>主要通过以下两种方式来对内存进行划分：</p><ul><li>分段</li><li>分页</li></ul><h3 id="内存分段" tabindex="-1"><a class="header-anchor" href="#内存分段" aria-hidden="true">#</a> 内存分段</h3><p>程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。<strong>不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来。</strong></p><p>分段机制下的虚拟地址由<strong>段选择因子(段号 + 特权等标志位) + 段内偏移量</strong>组成。</p><ul><li><strong>段选择因子</strong>就保存在段寄存器里面。段选择因子里面最重要的是<strong>段号</strong>，用作段表的索引。<strong>段表</strong>里面保存的是这个<strong>段的基地址、段的界限和特权等级</strong>等。</li><li>虚拟地址中的<strong>段内偏移量</strong>应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。</li></ul><img src="https://cdn.xiaolincoding.com//mysql/other/a9ed979e2ed8414f9828767592aadc21.png" alt="img" style="zoom:50%;"><p>分段的方法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但他也有一些不足之处：</p><ol><li>内存碎片(外部)</li><li>内存交换的效率低</li></ol><blockquote><p>内存碎片主要分为<strong>内部内存碎片</strong>、<strong>外部内存碎片</strong></p></blockquote><p>内存分段管理可以做到根据段的实际需求分配内存，需要多少就分配多少，所以不会产生内部内存碎片。</p><p>但是由于每个段的长度不固定，而且多个段未必能够恰好使用所有的内存空间，所以会产生多个不连续的小物理内存，也即会产生外部内存碎片。</p><p>解决「外部内存碎片」的问题就是<strong>内存交换</strong>。</p><blockquote><p>分段为什么会导致内存交换效率低的问题？</p></blockquote><ol><li>对于多进程的系统来说，用分段的方式，外部内存碎片很容易就产生，就不得不重新swap内存区域，这个过程会产生瓶颈。</li><li>因为硬盘的访问速度比内存慢太多，每一次内存交换，都需要把一大段连续的内存数据写到磁盘上。若内存交换时，交换的是一个占用很大内存空间的程序，那整个程序都会显得卡顿。</li></ol><h3 id="内存分页" tabindex="-1"><a class="header-anchor" href="#内存分页" aria-hidden="true">#</a> 内存分页</h3><p>分段的好处就是能产生连续的内存空间，但是会出现【外部碎片和内存交换的空间太大】的问题。</p><p><strong>分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小</strong>。这样一个连续并且尺寸固定的内存空间，我们叫<strong>页</strong>（<em>Page</em>）。在 Linux 下，每一页的大小为 <code>4KB</code>。</p><p>虚拟地址与物理地址之间通过<strong>页表</strong>来映射，页表存储在内存中，并由MMU来负责虚拟地址到物理地址的转换。</p><img src="https://cdn.xiaolincoding.com//mysql/other/08a8e315fedc4a858060db5cb4a654af.png" alt="img" style="zoom:67%;"><p>而当进程访问的虚拟地址在页表中查不到时，系统会产生一个<strong>缺页异常</strong>，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。</p><blockquote><p>分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？</p></blockquote><p><strong>采用了分页，页与页之间是紧密排列的，所以不会有外部碎片。</strong></p><p>但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对<strong>内存分页机制会有内部内存碎片</strong>的现象。</p><p>如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为<strong>换出</strong>（<em>Swap Out</em>）。一旦需要的时候，再加载进来，称为<strong>换入</strong>（<em>Swap In</em>）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，<strong>内存交换的效率就相对比较高。</strong></p><p>更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是<strong>只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。</strong></p><blockquote><p>分页机制下，虚拟地址和物理地址是如何映射的？</p></blockquote><p>在分页机制下，虚拟地址分为两部分，<strong>页号</strong>和<strong>页内偏移</strong>。页号作为页表的索引，<strong>页表</strong>包含物理页每页所在<strong>物理内存的基地址</strong>，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。</p><img src="https://cdn.xiaolincoding.com//mysql/other/7884f4d8db4949f7a5bb4bbd0f452609.png" alt="img" style="zoom:50%;"><p>总结一下，对于一个内存地址转换，其实就是这样三个步骤：</p><ul><li>把虚拟内存地址，切分成页号和偏移量；</li><li>根据页号，从页表里面，查询对应的物理页号；</li><li>直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。</li></ul><blockquote><p>简单的分页有什么缺陷吗？</p></blockquote><p>主要就是空间方面浪费较大，所以引入了一种<strong>多级页表</strong>的解决方案：</p><p>我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 <code>1024</code> 个页表（二级页表），每个表（二级页表）中包含 <code>1024</code> 个「页表项」，形成<strong>二级分页</strong>。如下图所示：</p><img src="https://cdn.xiaolincoding.com//mysql/other/19296e249b2240c29f9c52be70f611d5.png" alt="img" style="zoom:50%;"><blockquote><p>你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？</p></blockquote><p>当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，二级分页占用空间确实是更大了，但是，我们往往不会为一个进程分配那么多内存。</p><p>如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但<strong>如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表</strong>。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= <code>0.804MB</code>，这对比单级页表的 <code>4MB</code> 是不是一个巨大的节约？</p><p>那么为什么不分级的页表就做不到这样节约内存呢？</p><p>我们从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以<strong>页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项</strong>（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。</p><blockquote><p>两者的区别：</p></blockquote><p><strong>占用空间方面：</strong></p><ul><li><mark>单级内存分页将整个虚拟地址空间划分为了一个大的页表，如果页的大小很小，那么页表就很大</mark>，需要消耗大量的内存空间和访问开销。但是为了实现虚拟地址到物理地址的快速转换(使用偏移量计算出页表项的位置，复杂度降低到O1)，<mark>每个进程中都需要存储这样的一个页表。</mark></li><li>多级内存分页将整个虚拟地址空间划分为多个层次，每个层次使用不同大小的页表，可以有效的减少页表的大小和访问开销。</li></ul><p><strong>TLB(页表缓存)加速</strong></p><ul><li>使用TLB提高了系统的性能和效率：多级内存分页可以通过缓存和预取技术，提高页表项的访问效率，从而减少内存访问的延迟和开销。</li><li>TLB是位于CPU内部的页表缓存，有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。</li></ul><h3 id="段页式内存管理" tabindex="-1"><a class="header-anchor" href="#段页式内存管理" aria-hidden="true">#</a> 段页式内存管理</h3><p>内存分段和内存分页并不是对立的，他们是可以组合起来用在同一个系统中的，组合起来之后，通常称为<strong>段页式内存管理</strong>。</p><p>具体的：</p><ol><li>先将程序划分为多个由逻辑意义的段，</li><li>接着再把每个段划分为多个页，也就是对段划分出的连续空间，再划分固定大小的页。</li></ol><p>这样，逻辑地址结构就由<strong>段号、段内页号和页内偏移</strong>三部分组成。</p><p>用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：</p><figure><img src="https://cdn.xiaolincoding.com//mysql/other/8904fb89ae0c49c4b0f2f7b5a0a7b099.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>段页式地址变换中要得到物理地址须经过三次内存访问：</p><ul><li>第一次访问段表，得到页表起始地址；</li><li>第二次访问页表，得到物理页号；</li><li>第三次将物理页号与页内位移组合，得到物理地址。</li></ul><p>可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率</p><h3 id="linux内存布局" tabindex="-1"><a class="header-anchor" href="#linux内存布局" aria-hidden="true">#</a> Linux内存布局</h3><p><strong>Linux 系统主要采用了分页管理，但是由于 Intel 处理器的发展史，Linux 系统无法避免分段管理</strong>。于是 Linux 就把所有段的基地址设为 <code>0</code>，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被用于访问控制和内存保护。</p><p>另外，Linux 系统中虚拟空间分布可分为<strong>用户态</strong>和<strong>内核态</strong>两部分，其中用户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区。</p><blockquote><p>最后，说下虚拟内存有什么作用？</p></blockquote><ul><li>第一，虚拟内存<strong>可以使得进程对运行内存超过物理内存大小</strong>，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。</li><li>第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。</li><li>第三，<strong>页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等</strong>。在内存访问方面，操作系统提供了更好的安全性。</li></ul><h2 id="_2-2-malloc是如何分配内存的" tabindex="-1"><a class="header-anchor" href="#_2-2-malloc是如何分配内存的" aria-hidden="true">#</a> 2.2 malloc是如何分配内存的？</h2><hr><h1 id="三、进程、线程、协程" tabindex="-1"><a class="header-anchor" href="#三、进程、线程、协程" aria-hidden="true">#</a> 三、进程、线程、协程</h1><p>协程又叫纤程。</p><p>协程既不是进程也不是线程，协程仅是一个特殊的函数。协程、进程和线程不是一个维度的。</p><p>一个进程可以包含多个线程，一个线程可以包含多个协程。虽然一个线程内的多个协程可以切换但是这多个协程是串行执行的，某个时刻只能有一个线程在运行，没法利用CPU的多核能力。</p><p>协程与进程一样，也存在上下文切换问题。</p><ul><li><p>进程的切换者是操作系统，切换时机是根据操作系统自己的切换策略来决定的，用户是无感的。进程的切换内容包括<code>页全局目录</code>、内核栈和硬件上下文，切换内容被保存在内存中。 进程切换过程采用的是“从用户态到内核态再到用户态”的方式，<code>切换效率低</code>。</p></li><li><p>线程的切换者是操作系统，切换时机是根据操作系统自己的切换策略来决定的，用户是无感的。线程的切换内容包括内核栈和硬件上下文。线程切换内容被保存在内核栈中。线程切换过程采用的是“从用户态到内核态再到用户态”的方式，<code>切换效率中等</code>。</p></li><li><p>协程的切换者是<code>用户(编程者或应用程序)</code>,切换时机是用户自己的程序来决定的。协程的切换内容是硬件上下文，切换内存被保存在用自己的变量(用户栈或堆)中。协程的切换过程只有用户态(即没有陷入内核态),因此切换效率高。</p></li></ul><h2 id="_3-1-进程阻塞和线程阻塞的关系" tabindex="-1"><a class="header-anchor" href="#_3-1-进程阻塞和线程阻塞的关系" aria-hidden="true">#</a> 3.1 进程阻塞和线程阻塞的关系</h2><p>近期我已经反复强调了好几遍，<mark>在现代主流操作系统Windows和Linux上，不存在操作系统先选择要运行的进程、再从这个进程里面选择要运行的线程这种两层次调度机制</mark>。</p><p>**在Linux上，CPU调度单位就是进程，线程只是一种特殊进程，跟其他进程是平等的；**而Windows上调度单位就是线程（也就是操作系统原理里面讲的进程，Windows的process其实是与CPU调度无关的别的的东西）。所以为了文字清晰，下面我会统一用进程这个名词来表述操作系统的CPU调度单位。用户级线程这个名词，近年通常被叫做协程，下面我也用协程来称呼它，简便一些。</p><h3 id="线程系统调用阻塞是否导致进程阻塞的问题" tabindex="-1"><a class="header-anchor" href="#线程系统调用阻塞是否导致进程阻塞的问题" aria-hidden="true">#</a> 线程系统调用阻塞是否导致进程阻塞的问题</h3><blockquote><p>问题： 如果一个进程中的某一个线程调用了一个阻塞的系统调用函数后，那么该进程包括该进程中的其他所有线程也同时被阻塞 ？</p></blockquote><p>关于这个问题。网上有些解答似乎比较混乱。回答这个问题，首先要简单了解一下线程模型。 线程模型（下述对应关系为 “线程”对“内核调度实体”）</p><ul><li>多对1用户级线程模型 <ul><li>线程的创建、调度、同步，由所属进程的用户空间线程库实现。</li><li>用户态线程，对内核几乎是透明的（许多操作不需要内核接管）</li><li>但线程总要有一些操作经过内核，比如系统调用。</li><li>不需要频繁的内核态/用户态切换，处理速度非常快。</li><li><code>该模式下，当进程的某个线程，系统调用（比如I/O）阻塞时，该进程也会阻塞。</code></li><li>原因：该模式下，进程的所有线程，都对应一个内核调度实体（KES），并且内核不知道这个进程有哪些线程。KES无法将其他线程，调度到其他处理器上。该进程（所有的线程）被阻塞，直到本次系统调用（比如I/O）结束。</li></ul></li><li>1对1内核级线程模型**（目前（linux）基本上都采用一对一模型。）** <ul><li>每个用户线程都对应一个的内核调度实体。</li><li>内核会对每个线程进行调度，可以调度到其他处理器上。</li><li>线程每次操作会在用户态和内核态切换。</li><li>线程数量过多时，对系统性能有影响。</li></ul></li><li>多对多两级线程模型 <ul><li>每个用户线程拥有多个内核调度实体</li><li>多个用户线程也可以对应一个内核调度实体</li><li>实现该模型非常复杂。</li></ul></li></ul><p><strong>总结</strong> 线程系统调用阻塞时，在多对1用户级线程模型下，会导致所属进程阻塞。 在1对1或多对多模型下，不会导致该问题的发生。 如果是单进程单线程的话，不管哪个模型，都会阻塞的。</p><h2 id="_3-2-ssh连接上终端是否会创建进程" tabindex="-1"><a class="header-anchor" href="#_3-2-ssh连接上终端是否会创建进程" aria-hidden="true">#</a> 3.2 ssh连接上终端是否会创建进程</h2><ol><li><p>首先当你通过ssh连接上终端时，OS会自动为你启动一个shell/bash进程。</p><ol><li>可通过<code>echo $$</code>查看该进程的PID</li><li>这个Bash进程负责解析你输入的命令，并将其发送给相应的程序进行处理。Bash进程也负责执行脚本文件和管理各种操作，如环境变量、文件描述符等。</li></ol></li><li><p>当你在bash进程中输入命令并按下回车时，是否会创建进程取决于命令的类型</p><ol><li><p>一般来讲，有一些常见的创建新进程的命令和不创建新进程的命令：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>创建新进程的命令<span class="token punctuation">(</span>外部命令<span class="token punctuation">)</span>：\n\n<span class="token number">1</span>. 任何外部命令，如<span class="token variable"><span class="token variable">`</span><span class="token function">ls</span><span class="token variable">`</span></span>、<span class="token variable"><span class="token variable">`</span><span class="token function">cat</span><span class="token variable">`</span></span>、<span class="token variable"><span class="token variable">`</span><span class="token function">grep</span><span class="token variable">`</span></span>等。\n<span class="token number">2</span>. 执行脚本文件，例如<span class="token variable"><span class="token variable">`</span><span class="token function">bash</span> script.sh<span class="token variable">`</span></span>，其中<span class="token variable"><span class="token variable">`</span>script.sh<span class="token variable">`</span></span>是一个可执行的脚本文件。\n<span class="token number">3</span>. 启动新的子Shell，例如通过<span class="token variable"><span class="token variable">`</span><span class="token function">bash</span><span class="token variable">`</span></span>命令或直接输入<span class="token variable"><span class="token variable">`</span><span class="token function">bash</span><span class="token variable">`</span></span>来开启一个新的Bash子Shell。\n<span class="token number">4</span>. 后台执行的命令，使用<span class="token variable"><span class="token variable">`</span><span class="token operator">&amp;</span><span class="token variable">`</span></span>符号将命令放在后台运行，例如<span class="token variable"><span class="token variable">`</span><span class="token builtin class-name">command</span> <span class="token operator">&amp;</span><span class="token variable">`</span></span>。\n\n不创建新进程的命令（内建命令）：\nPs：不绝对，比如echo可能\n\n<span class="token number">1</span>. 环境相关命令，如<span class="token variable"><span class="token variable">`</span><span class="token builtin class-name">export</span><span class="token variable">`</span></span>、<span class="token variable"><span class="token variable">`</span><span class="token builtin class-name">unset</span><span class="token variable">`</span></span>，用于设置和取消环境变量。\n<span class="token number">2</span>. 目录相关命令，如<span class="token variable"><span class="token variable">`</span><span class="token builtin class-name">cd</span><span class="token variable">`</span></span>，用于改变当前工作目录。\n<span class="token number">3</span>. 文件操作命令，如<span class="token variable"><span class="token variable">`</span><span class="token builtin class-name">echo</span><span class="token variable">`</span></span>、<span class="token variable"><span class="token variable">`</span><span class="token function">touch</span><span class="token variable">`</span></span>，用于创建文件或打印输出。\n<span class="token number">4</span>. Shell控制命令，如<span class="token variable"><span class="token variable">`</span><span class="token builtin class-name">exit</span><span class="token variable">`</span></span>，用于退出当前Shell。\n<span class="token number">5</span>. Shell内置的流程控制命令，如<span class="token variable"><span class="token variable">`</span><span class="token keyword">if</span><span class="token variable">`</span></span>、<span class="token variable"><span class="token variable">`</span><span class="token keyword">for</span><span class="token variable">`</span></span>、<span class="token variable"><span class="token variable">`</span><span class="token keyword">while</span><span class="token variable">`</span></span>，用于条件判断和循环控制。\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>内部命令和外部命令的区别：</p><ul><li>内置命令与外部命令的主要区别在于内置命令被编译进Shell程序中，不需要启动新的进程，执行速度更快，但功能相对简单；而外部命令需要通过文件系统中的可执行文件来执行，需要启动新的进程，执行速度相对较慢，但功能更加强大。</li></ul><blockquote><ol><li>内部命令是Shell自带的命令，他们被编译进Shell程序中，当Shell解释器在执行命令时，不需要启动新的进程，直接在当前shell/bash进程执行命令。因此内部命令执行的速度比外部命令执行的速度较快，且不需要额外的系统调用。</li><li>外部命令是存储在文件系统的可执行文件，他们不属于Shell程序的一部分。当Shell解释器在执行外部命令的时候，需要启动新的进程来运行该命令。因此外部命令执行速度比内部命令慢，且需要额外的系统调用。</li></ol></blockquote></li><li><p>严格来讲，可通过<code>type</code>命令判断命令为内部命令或外部命令，从而得知是否会创建新的进程。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>$: <span class="token builtin class-name">type</span> <span class="token builtin class-name">cd</span>\n<span class="token builtin class-name">cd</span> is a shell <span class="token builtin class-name">builtin</span> <span class="token comment">#内建命令</span>\n\n$: <span class="token builtin class-name">type</span> <span class="token function">ls</span>\n<span class="token function">ls</span> is aliased to <span class="token variable"><span class="token variable">`</span><span class="token function">ls</span> <span class="token parameter variable">--color</span><span class="token operator">=</span>auto<span class="token variable">`</span></span>   <span class="token comment">#别名</span>\n\n$: <span class="token builtin class-name">type</span> <span class="token parameter variable">-a</span> <span class="token function">ls</span>  <span class="token comment">#查找出所有跟ls匹配的记录</span>\n<span class="token function">ls</span> is aliased to <span class="token variable"><span class="token variable">`</span><span class="token function">ls</span> <span class="token parameter variable">--color</span><span class="token operator">=</span>auto<span class="token variable">`</span></span> \n<span class="token function">ls</span> is /usr/bin/ls\n<span class="token function">ls</span> is /bin/ls\n<span class="token function">ls</span> is aliased to <span class="token variable"><span class="token variable">`</span><span class="token function">ls</span> <span class="token parameter variable">--color</span><span class="token operator">=</span>auto<span class="token variable">`</span></span>\n<span class="token function">ls</span> is /usr/bin/ls\n<span class="token function">ls</span> is /bin/ls\n\n$: <span class="token builtin class-name">type</span> <span class="token parameter variable">-t</span> /bin/ls\n<span class="token function">file</span>	<span class="token comment"># 外部命令</span>\n\n$: <span class="token builtin class-name">type</span> <span class="token function">ps</span>\n<span class="token function">ps</span> is /usr/bin/ps <span class="token comment"># 外部命令</span>\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li></ol><h2 id="_3-3-linux下进程的最大数、最大线程数、进程打开的文件数" tabindex="-1"><a class="header-anchor" href="#_3-3-linux下进程的最大数、最大线程数、进程打开的文件数" aria-hidden="true">#</a> 3.3 Linux下进程的最大数、最大线程数、进程打开的文件数</h2><h3 id="_3-3-1-进程的最大数" tabindex="-1"><a class="header-anchor" href="#_3-3-1-进程的最大数" aria-hidden="true">#</a> 3.3.1 进程的最大数</h3><h3 id="_3-3-2-一个进程最多能够创建多少个线程" tabindex="-1"><a class="header-anchor" href="#_3-3-2-一个进程最多能够创建多少个线程" aria-hidden="true">#</a> 3.3.2 一个进程最多能够创建多少个线程</h3><p>这个问题受限于两个因素：</p><ol><li><code>进程的虚拟内存空间上限</code>：创建一个线程就需要OS为其分配一个栈空间，如果线程数量越多，所需的栈空间就越大，那么需要的虚拟内存就越多。</li><li><code>系统参数限制</code>：Linux系统虽然没有指定单个进程最多能够创建的线程数量，但是指定了整个系统能够创建的最大线程个数。</li></ol><p><strong>① 考虑虚拟内存上限：</strong></p><ol><li><p>线程共享进程的内存空间，所以只需要给线程分配栈空间，那么创建一个线程默认需要分配多大的栈空间？</p><ol><li><code>ulimit -s</code>可以查看进程创建线程时默认分配的栈空间的大小。</li><li><code>ulimit -n</code>可以查看进程最大能够打开的文件数。</li><li><img src="'+p+'" alt="image-20230502174140436" loading="lazy">左侧是在我的服务器上执行的结果，大概8M。</li></ol></li><li><p>在linux系统中，32位、64位创建线程有什么不同？</p><blockquote><p>在Linux系统中，虚拟地址空间又被分为<u>内核空间</u>和<u>用户空间</u>，不同位数的系统，用户地址空间的范围不同</p><ul><li>在32位中，内核空间占1G，用户空间占3G</li><li>在64位中，内核空间和用户空间都是128T，分别占据整个内存空间的最高处和最低处，中间那部分空间未定义，我想应该是为了便于扩展使得两部分空间都是连续的。</li></ul></blockquote><ol><li>在32位中，一个进程能够创建的个数 ≈ 用户空间的大小(3G) / 默认分配的栈空间的大小(8M) = 375</li><li>在64位中，理论上，按照公式<code>一个进程能够创建的个数 ≈ 用户空间的大小(128T) / 默认分配的栈空间的大小(8M) = 16 000 000</code>能够创建一千六百万个线程，但是实际上会受系统的限制。</li></ol></li></ol><p><strong>② 考虑系统参数限制：</strong></p><p>在Linux系统中，会受以下参数的限制，具体参数的数值是我自己的服务器的参数：</p><ol><li><em><strong>/proc/sys/kernel/threads-max</strong></em>，表示系统支持的最大线程数，默认值是 <code>126462</code>；</li><li><em><strong>/proc/sys/kernel/pid_max</strong></em>，表示系统全局的 PID 号数值的限制，每一个进程或线程都有 ID，ID 的值超过这个数，进程或线程就会创建失败，默认值是 <code>4194304</code>；</li><li><em><strong>/proc/sys/vm/max_map_count</strong></em>，表示限制一个进程可以拥有的VMA(虚拟内存区域)的数量，具体什么意思我也没搞清楚，反正如果它的值很小，也会导致创建线程失败，默认值是 <code>65530</code>。</li></ol><p><strong>综上所述：</strong></p><ul><li>32位系统，默认最多能创建三百多个</li><li>64位系统，理论上是1600万个线程，但实际上还会受系统的参数和性能限制。</li></ul><h1 id="三、文件系统" tabindex="-1"><a class="header-anchor" href="#三、文件系统" aria-hidden="true">#</a> 三、文件系统</h1><h2 id="文件描述符" tabindex="-1"><a class="header-anchor" href="#文件描述符" aria-hidden="true">#</a> 文件描述符</h2><h3 id="基本概念" tabindex="-1"><a class="header-anchor" href="#基本概念" aria-hidden="true">#</a> 基本概念</h3><p><code>文件描述符（File descriptor）</code>形式上是一个非负整数，从 <strong>0</strong> 开始。进程使用文件描述符来表示一个打开的文件。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。操作系统为每一个进程维护了一个文件描述符<code>表</code>，表示该进程打开文件的记录表，而文件描述符实际上就是这张表的索引。当进程打开(open) 或新建(create) 文件时，内核会在该进程的文件列表中新增一个表项，同时返回一个文件描述符 —— 也就是新增表项的下标。</p><p><code>文件描述符的作用是什么？</code>每一个进程都有一个数据结构 <code>task_struct</code>，该结构体里有一个指向<code>「文件描述符数组」</code>的成员指针。<strong>该数组里列出这个进程打开的所有文件的文件描述符</strong>。数组的下标是文件描述符，是一个整数，而数组的内容是一个指针，指向内核中所有打开的文件的列表，也就是说内核可以通过文件描述符找到对应打开的文件。</p><p>每个进程默认都有 3 个文件描述符：0 (stdin)、1 (stdout)、2 (stderr)。</p><blockquote><ul><li><p>Linux系统上查看能够打开的最大文件数</p><ol><li><p><code>ulimit -n</code>命令用于查看或设置<code>当前用户或进程</code>能够打开的最大文件数（文件描述符限制）。</p></li><li><p><code>cat /proc/sys/fs/file-max</code>命令用于查看<code>整个系统</code>能够打开的最大文件数（系统级别的文件描述符限制）。这个值由操作系统内核设置，并适用于整个系统的所有用户和进程。</p></li></ol></li></ul><figure><img src="'+r+`" alt="image-20230429222638960" tabindex="0" loading="lazy"><figcaption>image-20230429222638960</figcaption></figure><ul><li>查看某个进程的文件描述符表的详细信息 <ul><li><code>ls -l /proc/&lt;PID&gt;/fd</code>：该命令将显示指定进程的文件描述符表的详细信息，包括每个文件描述符的相关信息和指向的文件路径。</li></ul></li></ul></blockquote><h3 id="fd与socket的关系" tabindex="-1"><a class="header-anchor" href="#fd与socket的关系" aria-hidden="true">#</a> fd与socket的关系</h3><p>socket可以用于同一台主机间不同进程的通信，也可以用于不同主机间的通信。一个socket包含地址、类型、通信协议等信息，通过<code>socket()</code>函数创建，该函数返回的就是 socket 对应的文件描述符 fd。</p><div class="language-c++ line-numbers-mode" data-ext="c++"><pre class="language-c++"><code>int socket (int domain, int type, int protocal)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>操作系统将 socket 映射到进程的一个文件描述符上，进程就可以通过读写这个文件描述符来和远程主机通信。</p><p>socket 是进程间通信规则的高层抽象，而 fd 提供的是底层的具体实现。socket 与 fd 是一一对应的。通过 socket 通信，实际上就是通过文件描述符 fd 读写文件。这也符合 Unix“一切皆文件”的哲学。</p><blockquote><p>在linux系统下，可以通过<code>ss -tunap</code>、<code>lsof -p 进程号</code>来查看系统或指定进程打开的文件和文件描述符。</p></blockquote><hr><h1 id="四、网络系统" tabindex="-1"><a class="header-anchor" href="#四、网络系统" aria-hidden="true">#</a> 四、网络系统</h1><h2 id="补充-网络包接收流程" tabindex="-1"><a class="header-anchor" href="#补充-网络包接收流程" aria-hidden="true">#</a> 补充：网络包接收流程</h2><figure><img src="`+h+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><ul><li>当<code>网络数据帧</code>通过网络传输到达网卡时，网卡会将网络数据帧通过<code>DMA的方式</code>放到<code>环形缓冲区RingBuffer</code>中。</li></ul><blockquote><p><code>RingBuffer</code>是网卡在启动的时候<code>分配和初始化</code>的<code>环形缓冲队列</code>。当<code>RingBuffer满</code>的时候，新来的数据包就会被<code>丢弃</code>。我们可以通过<code>ifconfig</code>命令查看网卡收发数据包的情况。其中<code>overruns</code>数据项表示当<code>RingBuffer满</code>时，被<code>丢弃的数据包</code>。如果发现出现丢包情况，可以通过<code>ethtool命令</code>来增大<code>RingBuffer</code>长度。</p></blockquote><ul><li>当<code>DMA操作完成</code>时，网卡会向CPU发起一个<code>硬中断</code>，告诉<code>CPU</code>有网络数据到达。CPU调用网卡驱动注册的<code>硬中断响应程序</code>。网卡硬中断响应程序会为网络数据帧创建内核数据结构<code>sk_buffer</code>，并将网络数据帧<code>拷贝</code>到<code>sk_buffer</code>中。然后发起<code>软中断请求</code>，通知<code>内核</code>有新的网络数据帧到达。</li></ul><blockquote><p><code>sk_buff</code>缓冲区，是一个维护网络帧结构的<code>双向链表</code>，链表中的每一个元素都是一个<code>网络帧</code>。虽然 TCP/IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而<code>无需进行数据复制</code>。</p></blockquote><ul><li>内核线程<code>ksoftirqd</code>发现有软中断请求到来，随后调用网卡驱动注册的<code>poll函数</code>，<code>poll函数</code>将<code>sk_buffer</code>中的<code>网络数据包</code>送到内核协议栈中注册的<code>ip_rcv函数</code>中。</li></ul><blockquote><p><code>每个CPU</code>会绑定<code>一个ksoftirqd</code>内核线程<code>专门</code>用来处理<code>软中断响应</code>。2个 CPU 时，就会有 <code>ksoftirqd/0</code> 和 <code>ksoftirqd/1</code>这两个内核线程。</p><p><strong>这里有个事情需要注意下：</strong> 网卡接收到数据后，当<code>DMA拷贝完成</code>时，向CPU发出<code>硬中断</code>，这时<code>哪个CPU</code>上响应了这个<code>硬中断</code>，那么在网卡<code>硬中断响应程序</code>中发出的<code>软中断请求</code>也会在<code>这个CPU绑定的ksoftirqd线程</code>中响应。所以如果发现Linux软中断，CPU消耗都<code>集中在一个核上</code>的话，那么就需要调整硬中断的<code>CPU亲和性</code>，来将硬中断<code>打散</code>到<code>不通的CPU核</code>上去。</p></blockquote><ul><li>在<code>ip_rcv函数</code>中也就是上图中的<code>网络层</code>，<code>取出</code>数据包的<code>IP头</code>，判断该数据包下一跳的走向，如果数据包是发送给本机的，则取出传输层的协议类型（<code>TCP</code>或者<code>UDP</code>)，并<code>去掉</code>数据包的<code>IP头</code>，将数据包交给上图中得<code>传输层</code>处理。</li></ul><blockquote><p>传输层的处理函数：<code>TCP协议</code>对应内核协议栈中注册的<code>tcp_rcv函数</code>，<code>UDP协议</code>对应内核协议栈中注册的<code>udp_rcv函数</code>。</p></blockquote><ul><li>当我们采用的是<code>TCP协议</code>时，数据包到达传输层时，会在内核协议栈中的<code>tcp_rcv函数</code>处理，在tcp_rcv函数中<code>去掉</code>TCP头，根据<code>四元组（源IP，源端口，目的IP，目的端口）</code>查找<code>对应的Socket</code>，如果找到对应的Socket则将网络数据包中的传输数据拷贝到<code>Socket</code>中的<code>接收缓冲区</code>中。如果没有找到，则发送一个<code>目标不可达</code>的<code>icmp</code>包。</li><li>内核在接收网络数据包时所做的工作我们就介绍完了，现在我们把视角放到应用层，当我们程序通过系统调用<code>read</code>读取<code>Socket接收缓冲区</code>中的数据时，如果接收缓冲区中<code>没有数据</code>，那么应用程序就会在系统调用上<code>阻塞</code>，直到Socket接收缓冲区<code>有数据</code>，然后<code>CPU</code>将<code>内核空间</code>（Socket接收缓冲区）的数据<code>拷贝</code>到<code>用户空间</code>，最后系统调用<code>read返回</code>，应用程序<code>读取</code>数据。</li></ul><h3 id="性能开销" tabindex="-1"><a class="header-anchor" href="#性能开销" aria-hidden="true">#</a> <strong>性能开销</strong></h3><p>从内核处理网络数据包接收的整个过程来看，内核帮我们做了非常之多的工作，最终我们的应用程序才能读取到网络数据。</p><p>随着而来的也带来了很多的性能开销，结合前面介绍的网络数据包接收过程我们来看下网络数据包接收的过程中都有哪些性能开销：</p><ul><li>应用程序通过<code>系统调用</code>从<code>用户态</code>转为<code>内核态</code>的开销以及系统调用<code>返回</code>时从<code>内核态</code>转为<code>用户态</code>的开销。</li><li>网络数据从<code>内核空间</code>通过<code>CPU拷贝</code>到<code>用户空间</code>的开销。</li><li>内核线程<code>ksoftirqd</code>响应<code>软中断</code>的开销。</li><li><code>CPU</code>响应<code>硬中断</code>的开销。</li><li><code>DMA拷贝</code>网络数据包到<code>内存</code>中的开销。</li></ul><h2 id="什么是零拷贝" tabindex="-1"><a class="header-anchor" href="#什么是零拷贝" aria-hidden="true">#</a> 什么是零拷贝？</h2><p>磁盘可以说是计算机系统最慢的硬件之一，读写速度相差内存10倍以上，所以针对优化磁盘的技术非常的多，比如零拷贝、直接 I/O、异步 I/O 等等，这些优化的目的就是为了提高系统的吞吐量，另外操作系统内核中的磁盘高速缓存区，可以有效的减少磁盘的访问次数。</p><p>我们就以「文件传输」作为切入点，来分析 I/O 工作方式，以及如何优化传输文件的性能。</p><h3 id="dma技术" tabindex="-1"><a class="header-anchor" href="#dma技术" aria-hidden="true">#</a> DMA技术</h3><p>在没有DMA技术前，I/O的过程是这样的：</p><ol><li>CPU 发出对应的指令给磁盘控制器，然后返回；</li><li>磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个<strong>中断</strong>；</li><li>CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。</li></ol><figure><img src="'+u+`" alt="I_O中断" tabindex="0" loading="lazy"><figcaption>I_O中断</figcaption></figure><p>从上图可以看到，整个数据的传输过程都需要CPU亲自参与搬运数据，而且这个过程，CPU不能做其他事情。</p><p>什么是 DMA 技术？简单理解就是，<strong>在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务</strong>。</p><h4 id="使用dma控制器进行数据传输" tabindex="-1"><a class="header-anchor" href="#使用dma控制器进行数据传输" aria-hidden="true">#</a> 使用DMA控制器进行数据传输</h4><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/零拷贝/DRM I_O 过程.png" alt="img" style="zoom:50%;"><p>具体过程：</p><ul><li>用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；</li><li>操作系统收到请求后，进一步将 I/O 请求发送 DMA，<code>然后让 CPU 执行其他任务</code>；</li><li>DMA 进一步将 I/O 请求发送给磁盘；</li><li>磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；</li><li><strong><mark>DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务</mark></strong>；</li><li>当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；</li><li>CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；</li></ul><p>可以看到，<mark>CPU 不再参与「将数据从磁盘控制器缓冲区搬运到内核空间」的工作，这部分工作全程由 DMA 完成</mark> 。但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。</p><p>早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。</p><h3 id="传统的文件传输-4拷贝、4切换" tabindex="-1"><a class="header-anchor" href="#传统的文件传输-4拷贝、4切换" aria-hidden="true">#</a> 传统的文件传输(4拷贝、4切换)</h3><p>传统 I/O 的工作方式是，<mark>数据读取和写入是从用户空间到内核空间<code>来回复制</code>，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入</mark>。</p><p>代码通常如下，一般会需要两个系统调用：</p><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token function">read</span><span class="token punctuation">(</span>file<span class="token punctuation">,</span> tmp_buf<span class="token punctuation">,</span> len<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">write</span><span class="token punctuation">(</span>socket<span class="token punctuation">,</span> tmp_buf<span class="token punctuation">,</span> len<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>虽然就两行代码，但是这里面发生了不少事情。</p><figure><img src="`+g+'" alt="传统文件传输" tabindex="0" loading="lazy"><figcaption>传统文件传输</figcaption></figure><ul><li>期间发生了**<code>4</code>**次用户态与内核态的上下文切换。</li><li>一次是 <code>read()</code> ，一次是 <code>write()</code>，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。</li></ul><p>其次，还<strong>发生了 4 次数据拷贝</strong>，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：</p><ul><li><em>第一次拷贝</em>，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。</li><li><em>第二次拷贝</em>，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。</li><li><em>第三次拷贝</em>，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。</li><li><em>第四次拷贝</em>，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。</li></ul><p>过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。</p><p>总结来说，传统的数据传送所消耗的成本：<code>4 次拷贝，4 次上下文切换</code>。 4 次拷贝，其中两次是 DMA copy，两次是 CPU copy。</p><blockquote><p>如何优化文件传输的性能？</p><ol><li>减少【用户态与内核态的上下文切换】的次数 <ol><li>减少系统调用的次数。</li></ol></li><li>减少【数据拷贝】的次数 <ol><li>四次拷贝中，从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到socket的缓冲区里，这两个拷贝过程没有必要。</li></ol></li></ol></blockquote><h3 id="如何实现零拷贝" tabindex="-1"><a class="header-anchor" href="#如何实现零拷贝" aria-hidden="true">#</a> 如何实现零拷贝？</h3><p>目的：减少 IO 流程中不必要的拷贝，当然零拷贝需要 OS 支持，也就是需要 kernel 暴露 API。</p><h4 id="mmap内存映射-3拷贝、4切换" tabindex="-1"><a class="header-anchor" href="#mmap内存映射-3拷贝、4切换" aria-hidden="true">#</a> mmap内存映射(3拷贝、4切换)</h4><p>硬盘上文件的位置和应用程序缓冲区(application buffers)进行映射（建立一种一一对应关系），由于<code>mmap()</code>将文件直接映射到用户空间，所以实际文件读取时根据这个映射关系，<strong>直接将文件从硬盘拷贝到用户空间，只进行了一次数据拷贝，不再将文件内容从硬盘拷贝到内核空间的一个缓冲区</strong>。</p><ol><li>应用进程调用了 <code>mmap()</code> 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；</li><li>应用进程再调用 <code>write()</code>，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；</li><li>最后，用DMA把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里。</li></ol><p><code>mmap</code>内存映射将会经历：</p><ul><li>3 次拷贝: 1 次 cpu copy，2 次 DMA copy；</li><li>4 次上下文切换，调用 mmap 函数 2 次，write 函数 2 次。</li><li><img src="'+b+'" alt="image-20230322205221612" tabindex="0" loading="lazy"><figcaption>image-20230322205221612</figcaption></li></ul><h4 id="sendfile-3-2拷贝-2切换" tabindex="-1"><a class="header-anchor" href="#sendfile-3-2拷贝-2切换" aria-hidden="true">#</a> sendfile(3/2拷贝，2切换)</h4><p>linux 2.1 支持的 sendfile</p><p>当调用 <code>sendfile()</code>时，</p><ol><li>DMA将磁盘数据复制到 kernel buffer，</li><li>然后将内核中的 kernel buffer 直接拷贝到 socket buffer <ol><li>如果设备支持(网卡支持 SG-DMA) <ol><li><strong>数据并未被真正复制到 socket 关联的缓冲区内</strong>。取而代之的是，<code>只有记录数据位置和长度的描述符被加入到 socket 缓冲区中</code>。</li><li>DMA模块将数据直接从内核缓冲区传递给协议引擎，从而消除了遗留的最后一次复制。但是要注意，这个需要 DMA 硬件设备支持。</li></ol></li><li>如果设备不支持 <ol><li>如果不支持，CPU就必须介入进行拷贝。</li></ol></li></ol></li><li>一旦数据全都拷贝到 socket buffer，sendfile()系统调用将会 return，代表数据转化的完成。socket buffer 里的数据就能在网络传输了。</li></ol><p>sendfile 会经历：</p><ul><li>3（2，如果硬件设备支持）次拷贝，1（0，如果硬件设备支持）次 CPU copy， 2 次 DMA copy；</li><li>2 次上下文切换</li></ul><table><thead><tr><th style="text-align:center;">如果设备不支持</th><th style="text-align:center;">如果设备支持</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/零拷贝/senfile-3次拷贝.png" alt="img" loading="lazy"></td><td style="text-align:center;"><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/零拷贝/senfile-零拷贝.png" alt="img" loading="lazy"></td></tr></tbody></table><h4 id="splice-2拷贝、2切换" tabindex="-1"><a class="header-anchor" href="#splice-2拷贝、2切换" aria-hidden="true">#</a> splice(2拷贝、2切换)</h4><p>Linux 从 2.6.17 支持</p><p>splice 数据从磁盘读取到 OS 内核缓冲区后，在内核缓冲区直接可将其转成内核空间其他数据 buffer，而不需要拷贝到用户空间。</p><p>如下图所示，从磁盘读取到内核 buffer 后，在内核空间直接与 socket buffer 建立 pipe 管道。</p><p>和 sendfile()不同的是，<code>splice()不需要硬件支持</code>。</p><p>注意 splice 和 sendfile 的不同:</p><ul><li>sendfile 是 DMA 硬件设备不支持的情况下将磁盘数据加载到 kernel buffer 后，需要一次 CPU copy，拷贝到 socket buffer。而 splice 是更进一步，连这个CPU copy也不需要了，直接将两个内核空间的 buffer 进行 pipe。</li></ul><p>splice 会经历 2 次拷贝:</p><ul><li>0 次 cpu copy, 2 次 DMA copy</li><li>2 次上下文切换</li><li><img src="'+m+`" alt="image-20230322210046826" tabindex="0" loading="lazy"><figcaption>image-20230322210046826</figcaption></li></ul><h4 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h4><p>最早的零拷贝定义，来源于 Linux 2.4 内核新增 sendfile 系统调用，提供了零拷贝。磁盘数据通过 DMA 拷贝到内核 态 Buffer 后，直接通过 DMA 拷贝到 NIO Buffer(socket buffer)，无需 CPU 拷贝。这也是零 拷贝这一说法的来源。这是真正操作系统 意义上的零拷贝(也就是狭义零拷贝)。 随着发展，零拷贝的概念得到了延伸，就是目前的减少不必要的数据拷贝都算作零拷贝 的范畴。</p><h3 id="零拷贝的应用" tabindex="-1"><a class="header-anchor" href="#零拷贝的应用" aria-hidden="true">#</a> 零拷贝的应用</h3><ol><li>Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置<code>http{sendfile on;}</code><ol><li>设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。</li><li>设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。</li></ol></li><li>Kafka好像也利用了<strong>零拷贝</strong>技术，从而大幅提升了 I/O 的吞吐率。</li></ol><hr><h2 id="pagecache-有什么作用" tabindex="-1"><a class="header-anchor" href="#pagecache-有什么作用" aria-hidden="true">#</a> PageCache 有什么作用？</h2><p>回顾前面说道文件传输过程，其中第一步<code>都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是磁盘高速缓存（PageCache）</code>。</p><p>由于<strong>零拷贝使用了 PageCache 技术</strong>，可以使得零拷贝进一步提升了性能，我们接下来看看 PageCache 是如何做到这一点的:</p><p>读写磁盘相比读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘里的数据搬运到内存里，这样就可以用读内存替换读磁盘。</p><p>但是，内存空间远比磁盘要小，内存注定只能拷贝磁盘里的一小部分数据。那问题来了，选择哪些磁盘数据拷贝到内存呢？</p><p>我们都知道程序运行的时候，具有「局部性」，所以通常，<code>刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用 **PageCache 来缓存最近被访问的数据**，当空间不足时淘汰最久未被访问的缓存</code>。</p><p>所以，<code>读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中</code>。</p><p>还有一点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，<strong>PageCache 使用了「预读功能」</strong>。</p><p>比如，假设 read 方法每次只会读 <code>32 KB</code> 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低，如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。</p><p>所以，<code>PageCache 的优点主要是两个</code>：</p><ul><li><code>**缓存最近被访问的数据；**</code></li><li><code>**预读功能；**</code></li></ul><p>这两个做法，将大大提高读写磁盘的性能。</p><p><strong>但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能</strong></p><p>这是因为如果你有很多 GB 级别文件需要传输，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，于是 PageCache 空间很快被这些大文件占满。</p><p>另外，由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来 2 个问题：</p><ul><li>PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了；</li><li>PageCache 中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次；</li></ul><p>所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。</p><hr><h2 id="大文件传输用什么方式实现" tabindex="-1"><a class="header-anchor" href="#大文件传输用什么方式实现" aria-hidden="true">#</a> 大文件传输用什么方式实现？</h2><p>那针对大文件的传输，我们应该使用什么方式呢？</p><table><thead><tr><th>原来的例子(会阻塞)</th><th>利用异步IO解决阻塞</th></tr></thead><tbody><tr><td>1. 当调用 read 方法时，<code>会阻塞着</code>，此时内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好；<br>2. 内核收到 I/O 中断后，就将数据从磁盘控制器缓冲区拷贝到 PageCache 里；<br>3. 最后，内核再把 PageCache 中的数据拷贝到用户缓冲区，于是 read 调用就正常返回了。</td><td>1. 前半部分，内核向磁盘发起读请求，但是可以<strong>不等待数据就位就可以返回</strong>，于是进程此时可以处理其他任务；<br>2. 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的<strong>通知</strong>，再去处理数据；</td></tr><tr><td><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/零拷贝/阻塞 IO 的过程.png" alt="img"></td><td><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/零拷贝/异步 IO 的过程.png" alt="img"></td></tr><tr><td>当调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回</td><td>异步 I/O 并没有涉及到 PageCache，所以使用异步 I/O 就意味着要绕开 PageCache</td></tr></tbody></table><p><code>绕开 PageCache 的 I/O 叫直接 I/O，使用 PageCache 的 I/O 则叫缓存 I/O</code>。</p><p>通常，对于磁盘，异步 I/O 只支持直接 I/O。</p><p>前面也提到，大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache。</p><p>于是，<strong><code>在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术</code></strong>。</p><p><strong>直接 I/O 应用场景常见的两种：</strong></p><ul><li>应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，<strong>默认是不开启</strong>；</li><li>传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。</li></ul><p>另外，由于直接 I/O 绕过了 PageCache，就无法享受内核的这两点的优化：</p><ul><li>内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「<strong>合并</strong>」成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作；</li><li>内核也会「<strong>预读</strong>」后续的 I/O 请求放在 PageCache 中，一样是为了减少对磁盘的操作；</li></ul><p>于是，传输大文件的时候，使用「异步 I/O + 直接 I/O」了，就可以无阻塞地读取文件了。</p><p>所以，传输文件的时候，我们要根据文件的大小来使用不同的方式：</p><ul><li><mark>传输大文件的时候，使用「异步 I/O + 直接 I/O」</mark>；</li><li><mark>传输小文件的时候，则使用「零拷贝技术」</mark>；</li></ul><p>在 nginx 中，我们可以用如下配置，来根据文件的大小来使用不同的方式：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>location /video/ { 
    sendfile on; 
    aio on; 
    directio 1024m; 
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>当文件大小大于 <code>directio</code> 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。</p><hr><h2 id="java生态圈中的零拷贝" tabindex="-1"><a class="header-anchor" href="#java生态圈中的零拷贝" aria-hidden="true">#</a> Java生态圈中的零拷贝</h2><p>Linux 提供的零拷贝技术 Java 并不是全支持，支持 2 种(内存映射 mmap、sendfile)；</p><p><strong>NIO提供的内存映射 MappedByteBuffer</strong></p><p>NIO中的FileChannel.map()方法其实就是采用了操作系统中的内存映射方式，底层就是调用Linux mmap()实现的。</p><p>将内核缓冲区的内存和用户缓冲区的内存做了一个地址映射。这种方式适合读取大文件，同时也能对文件内容进行更改，但是如果其后要通过SocketChannel发送，还是需要CPU进行数据的拷贝。</p><p><strong>NIO提供的sendfile</strong></p><p>Java NIO 中提供的 FileChannel 拥有 transferTo 和 transferFrom 两个方法，可直接把 FileChannel 中的数据拷贝到另外一个 Channel，或者直接把另外一个 Channel 中的数据拷贝到 FileChannel。该接口常被用于高效的网络 / 文件的数据传输和大文件拷贝。在操作系统支持的情况下，通过该方法传输数据并不需要将源数据从内核态拷贝到用户态，再从用户态拷贝到目标通道的内核态，同时也避免了两次用户态和内核态间的上下文切换，也即使用了“零拷贝”，所以其性能一般高于 Java IO 中提供的方法。</p><p><strong>Netty的零拷贝实现</strong></p><p>Netty 的零拷贝主要包含三个方面：</p><p>在网络通信上，Netty 的接收和发送 ByteBuffer 采用 DIRECT BUFFERS，使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行 Socket 读写，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。</p><p>在缓存操作上，Netty提供了CompositeByteBuf类，它可以将多个ByteBuf合并为一个逻辑上的ByteBuf，避免了各个ByteBuf之间的拷贝。</p><p>通过wrap操作，我们可以将byte[]数组、ByteBuf、 ByteBuffer 等包装成一个 Netty ByteBuf对象，进而避免了拷贝操作。</p><p>ByteBuf支持slice 操作，因此可以将ByteBuf分解为多个共享同一个存储区域的ByteBuf，避免了内存的拷贝。</p><p>在文件传输上，Netty 的通过FileRegion包装的FileChannel.tranferTo实现文件传输，它可以直接将文件缓冲区的数据发送到目标 Channel，避免了传统通过循环 write 方式导致的内存拷贝问题。</p><hr><h2 id="高性能网络模式-reactor-和-proactor" tabindex="-1"><a class="header-anchor" href="#高性能网络模式-reactor-和-proactor" aria-hidden="true">#</a> 高性能网络模式：Reactor 和 Proactor</h2><p>reids、nginx、netty都用到了Reactor模式</p><hr><h1 id="常见参数" tabindex="-1"><a class="header-anchor" href="#常见参数" aria-hidden="true">#</a> 常见参数</h1><h2 id="硬件" tabindex="-1"><a class="header-anchor" href="#硬件" aria-hidden="true">#</a> 硬件</h2><p><strong>主频</strong></p><p>CPU 每秒钟能执行多少个时钟周期。</p><ul><li>CPU 的主频越高，每个时钟周期的时间就越短，CPU 的处理能力也就越强</li></ul><p><strong>时钟周期</strong></p><p>CPU 的时钟周期是指 CPU 执行一个基本操作所需要的时间</p><h2 id="网络性能指标" tabindex="-1"><a class="header-anchor" href="#网络性能指标" aria-hidden="true">#</a> 网络性能指标</h2><ul><li><em>带宽</em>，表示链路的最大传输速率，单位是 b/s （比特 / 秒），带宽越大，其传输能力就越强。</li><li><em>延时</em>，表示请求数据包发送后，收到对端响应，所需要的时间延迟。不同的场景有着不同的含义，比如可以表示建立 TCP 连接所需的时间延迟，或一个数据包往返所需的时间延迟。</li><li><em>吞吐率</em>，表示<strong>单位时间内成功传输的数据量</strong>，单位是 b/s（比特 / 秒）或者 B/s（字节 / 秒），吞吐受带宽限制，带宽越大，吞吐率的上限才可能越高。</li><li><em>PPS</em>，全称是 Packet Per Second（包 / 秒），表示<strong>以网络包为单位的传输速率</strong>，一般用来评估<code>系统对于网络的转发能力</code>。</li></ul><p>除了以上这四种基本的指标，还有一些其他常用的性能指标，比如：</p><ul><li><em>网络的可用性</em>，表示网络能否正常通信；</li><li><em>并发连接数</em>，表示 TCP 连接数量；</li><li><em>丢包率</em>，表示所丢失数据包数量占所发送数据组的比率；</li><li><em>重传率</em>，表示重传网络包的比例；</li></ul>`,357),C=[f];function P(v,x){return a(),i("div",null,C)}const A=e(k,[["render",P],["__file","操作系统.html.vue"]]);export{A as default};
